\section{Classification of shadow operations}
\label{ch:sieve:sec:label}
In this section, we first
discuss the main challenges and design choices of making
\tool\ correctly label commutative shadow operations
either strongly or weakly consistent in Section~\ref{ch:sieve:sec:label:overview}.
Then, we expand in Section~\ref{ch:sieve:sec:label:static} on how to leverage a static analysis
to enable an efficient and less pessimistic runtime labeling. Finally,
we explain how the runtime component takes advantage of the information generated
in the static phase in Section~\ref{ch:sieve:sec:label:runtime}.

\subsection{Overview}
\label{ch:sieve:sec:label:overview}
%Shadow operations are labeled at runtime according to 
%their inherent invariant preservation properties. 
As mentioned in Section~\ref{ch:sieve:sect:overview}, 
a possible solution would be to statically compute the combinations 
of operation types, parameters, and initial states that 
generate invariant safe shadow operations.
This can be done by performing a
weakest precondition computation---a common technique from Programming Languages and Verification research for
which some tool support already exists---which enables us to 
statically compute, given the code of each operation and the application-specific invariants (which are inserted as postconditions),
a precondition over the initial state and operation parameters
that ensures the invariant safety property. The advantage of
using the weakest precondition concept is to allow us to have
a dynamic and optimistic labeling result at runtime, i.e., shadow operations
need to execute with coordination only if the respective precondition evaluates to {\tt FALSE}.

Despite the above benefit, however, the weakest precondition
computation raises the following two important problems. First, there is a scalability problem, which is exemplified by
the following hypothetical code for the generator operation,
assuming an invariant that the state variable {\tt x} should
be non-negative. (For simplicity, we write conventional Java code accessing
variable {\tt x} instead of SQL.)

\begin{verbatim}
void generator(string s) {
  if (SHA-1(s)==SOME_CONSTANT) {
     if (x>=10){
        x -= 10;
     }
  } else
     x +=10;
}
\end{verbatim}

The problem with this code is that 
a weakest precondition analysis to determine which values of {\tt s} lead to a
negative (non-invariant safe) delta over {\tt x} is computationally
infeasible, since it amounts to inverting a hash function.
As such, we would end up conservatively labeling the shadow operations
generated by this code as red (i.e., the weakest precondition would
be {\tt FALSE}). Even though this is an extreme example, it highlights the difficulty
in handling complex conditions over the input,
even when the side effects are simple.
In particular, there are only
three patterns of side effects produced by this generator, regardless
of the inputs provided to the generator operation. Based on this observation,
to simplify the weakest precondition computation
and to minimize the space of strongly consistent shadow operations,
our static analysis is conducted over the
set of possible sequences of CRDT operations that can be generated,
which is the same as saying that we analyze all possible 
shadow operations. We call each possible sequence of shadow operations 
that can be generated by a given generator operation a \texttt{template}. 
In the above example, there are only three sequences of shadow operations that can be
generated: the empty sequence, adding a delta of 10, and adding
a delta of $-10$. From these three possible sequences, only a delta
of $-10$ leads to a weakest precondition of {\tt FALSE}, i.e.,
is always non-invariant safe. The remaining
ones have a weakest precondition of {\tt TRUE}. \changebars{(Note that
the precondition could also be parameterized over the parameters of shadow operations.)}{}

\begin{figure*}[t!]
\begin{minipage}[b]{.42\textwidth}
\subfloat[Original code]{
\pseudocodeinput[breaklines=true,mathescape=true]{pseudocode/sieve/forLoopExample.txt}
\label{fig:codeOriginal}
}
\end{minipage}
\begin{minipage}[b]{.56\textwidth}
\subfloat[Possible corresponding shadow template]{
\pseudocodeinput[breaklines=true,mathescape=true]{pseudocode/sieve/forLoopShadowExample.txt}
\label{fig:codeShadow}
}
\end{minipage}
\caption{Code snippet of a transaction and a possible template for the corresponding shadow operation.}
\label{fig:codeDoBuy}
\end{figure*}


The second challenge that needs to be overcome is related to handling
loops. %, given the length of the possible sequences of CRDT operation
%that can be generated. 
The generator code in Figure~\ref{fig:codeDoBuy}\subref{fig:codeOriginal} illustrates that the number of
iterations in the loop can be unbounded, which in turn leads to an
unbounded number of sequences of CRDT operations in the shadow operation. To abstract
this, we could produce a template that preserves the loop structure, such as the one in Figure~\ref{fig:codeDoBuy}\subref{fig:codeShadow}. 
However, when computing a weakest precondition over this piece of code,
verification tools face a scalability problem, which is overcome
%In fact,
%verification tools face this type of scalability problem. 
%For instance, a possible solution to handling loops would be to compute a
%weakest precondition for the adapted template code in
%Figure~\ref{fig:codeShadow}. 
%Therefore, given the difficulty
%in scaling the analysis, verification tools
by requiring the programmer to specify loop invariants that guide the
computation of this weakest precondition~\cite{Kuncak2007Jahob}. Again,
this would represent an undesirable
programmer intervention.

To address this challenge, we note that in many cases (including all
applications that we analyzed), loop iterations are independent, in the
sense that the parts of the state modified in each iteration are disjoint.
Again, this is illustrated by the example in Figure~\ref{fig:codeDoBuy}, where
the loop is used to iterate over a set of items, and each iteration
only modifies the state of the item being iterated.

This iteration independence property enables us to significantly simplify the handling of loops. 
In particular, when generating the weakest precondition associated with a loop, we only have to
consider the CRDT operations invoked in two sets of control flow paths, one where the code within the loop is
never executed, and another with all possible control flow paths 
when the loop is executed and iteration repetitions are eliminated. (We will explain in detail
how to handle loops using an example in the following subsection.)
This condition can then be validated against each individual iteration of the loop at runtime and,
given the independence property, this validation will be valid for the entire loop execution.

In our current framework, the iteration independence
property is validated manually. In all our case-study applications, it
was straightforward to see that this property was met at all times.
We leave the automation of this step as future work.


\if 0

A shadow operation only exercises a single execution path
of the original transaction, while a template captures all distinct possible execution paths
of the original transaction. The different
granularity between templates and shadow operations introduces two challenges. 
First, because templates encode all possible execution paths, the size of
computed weakest precondition grows significantly as the number of possible 
execution paths grows, for instance, in the presence
of the mix of conditional branches~\cite{Flanagan2001CompactVC}. 
As we discuss in the evaluation section, some transactions exhibit a complex structure
with many possibles distinct paths. In this cases,
a big precondition is generated, which leads to more complex runtime verification\cheng{~Check two
things: a) time-consuming to evaluate (heavy-weight); and b) will be simplified to false (not minimizing
the strong consistency part).}. 
The second issue, is that these larger weakest preconditions are harder to simplify, which may lead
\tool\ to conservatively reduce these weakest preconditions to \texttt{false}, potentially leading to an
increase in the number of shadow operations (unnecessarily) labeled as red, which leads to 
performance penalties on the system. Therefore, producing
fine-grained templates is crucial to ensure low overhead and higher performance.

To achieve this, we have changed the strategy to generate templates, such that
original transaction maps to a set of templates, where each template represents one
execution path of the original transaction. Therefore, shadow operations become 
instantiations of the template that matches to the concrete execution path for the corresponding
original transaction. This requires the runtime component of \tool\ to keep track of the execution
path of operations, as to match the current shadow operation to the corresponding template.
We discuss in the following section how this is achieved in practice.


\subsubsection{Optimized Design}

\fi


%\begin{figure}[t!]
%\centering
%\begin{minipage}{.48\textwidth}
%\pseudocodeinput[breaklines=true,mathescape=true]{code/forLoopExample.txt}
%\end{minipage}
%\caption{Code snippet of a transaction and its shadow template.}
%\caption{Code snippet of a transaction with loop}
%\label{fig:codeOriginal}
%\end{figure}

\begin{table}[t]
\centering
\begin{tabular}{|c|c|}
\hline
\ \ \ \ \ Sequential path \ \ \ \ \ & \ \ \ \ \  Description \ \ \ \ \ \\\hline
$2\cdot3\cdot4\cdot2$ & only if\\
$2\cdot3\cdot6\cdot2$ & only else\\
$2\cdot3\cdot4\cdot2\cdot3\cdot6\cdot2$ & else follows if\\
$2\cdot3\cdot6\cdot2\cdot3\cdot4\cdot2$ & if follows else\\
\hline
\end{tabular}
\caption{Distinct sequential paths obtained for the transaction in Figure~\ref{fig:codeDoBuy}\protect\subref{fig:codeOriginal}.}
\label{tab:pathreduce}
\end{table}

\subsection{Generating templates and weakest preconditions}
\label{ch:sieve:sec:label:static}
Instead of reasoning about the generator code, our analysis
is simplified by reasoning about the side effects of each code path
taken by the generator operation. Furthermore, we can cut the number of
possible code paths by eliminating code sections that are repeated due to loops.
%Instead of reasoning about all possible sequences of CRDT operations, our
%analysis is slightly simplified by equivalently reasoning about all
%possible code paths taken by the generator operation.

To perform this analysis,
we require an algorithm for extracting the set of 
sequential paths of a transaction and eliminating loop repetition.
The high level idea of this algorithm is to split branch statements
and replace loops with all non-repeating combinations of branches that
can be taken within a loop. The algorithm works as follows.
First, for every transaction,
we create its path abstraction, which is a regular expression
encoding all control flow information within
that transaction. In the example shown in Figure~\ref{fig:codeDoBuy}\subref{fig:codeOriginal}, 
its path abstraction is $2\cdot(3\cdot(4|6)\cdot2)*$, where numbers
represent the statement identifiers shown in the figure, $\cdot$ concatenates two sequential statements,
$|$ is a binary operator that indicates that the statements at its two sides
are in alternative branches, and $*$ represents repetition within a loop.
Second, we recursively apply the following two steps to simplify a path abstraction until 
it is sequential (i.e., no $*$ and $|$). For a path abstraction containing $*$, we create two duplicated
abstractions, where one excludes the entire loop, and the other simplifies the loop into its body.
For a path abstraction containing the operator $|$, we create two duplicated
path abstractions, where one excludes the right operand and the other
excludes the left operand. Additionally, if such $|$ is affected by
a $*$, then we have to create another  path abstraction combining
both alternatives, i.e., where the if and the else sides are executed
sequentially. 

In the previous example, the set of sequential paths that is produced
is shown in Table~\ref{tab:pathreduce}. By ignoring the read-only path
where the loop is not executed, we only consider four cases, namely only the if or the else path, and the two sequences including both if and else. 
Because of the loop independence property, these
cases are able to capture all relevant sequences of shadow operations.
Note that we would only require considering one of the two
orderings for the if and the else code within the loop,
since their side effects commute, but taking both orderings into
account simplifies the runtime matching of an execution to its
corresponding path.

Given a set of sequential paths for a transaction, creating shadow operation templates become
straightforward. For each path, we collect a sequence of
statements specified by the identifiers in the abstraction from the corresponding control flow graph.
Then, we translate every database function call into either a CRDT operation by following
the instructions stated in Section~\ref{ch:sieve:sect:commute}, or a no-op operation (for read-only queries). 
Finally, all these CRDT operations are packed into a function, which
denotes the shadow operation template. These CRDT operations are parameterized
by their respective arguments, and the
static analysis computes a weakest precondition over
these arguments for the template to be invariant safe. We did not devise an algorithm
to compute weakest preconditions, instead, we rely on a verification tool called Jahob~\cite{Kuncak2007Jahob} to do
this job. The input
fed into Jahob is comprised by a set of templates along with their preconditions and postconditions, which
are automatically extracted by the static analysis code.

The final output from the static analysis is a dictionary consisting of
a set of $\langle key, value\rangle$ pairs, one for each previously generated shadow operation template, where $key$
is the unique identifier of the template,
and $value$ is the weakest precondition for the template.
%The unique identifier of the template is generated by concatenating 
%the signatures of CRDT operations only.
The unique identifier of the template encodes the set of possible paths 
using signatures of CRDT operations in a restricted form of regular
expression.

\subsection{Runtime evaluation}
\label{ch:sieve:sec:label:runtime}
%\paragraph{Template/shadow operation matching.}
Determining if a generated shadow operation is \red\ or \blue\ consists
of two steps: a) fetching its weakest precondition by
matching this operation to its corresponding
template; and b) evaluating the condition by substituting
variables in the condition with values carried by this operation.
 
\paragraph{Template/shadow operation matching.}
At runtime, we must lookup in the
dictionary created during the static analysis the template corresponding to 
each shadow operation as it is produced.

The challenge with performing this lookup is that it requires determining
the identifier of the shadow operation corresponding to the path taken, and this
must be done by taking into account \textit{only} the 
operations that are controlled by the runtime, i.e., the CRDT operations.
This explains why the dictionary keys consist only of CRDT operations.
With the shadow operation identifier, matching the path taken at runtime with the keys present 
in the dictionary is done efficiently by using a search tree.



\if 0

By looking up the weakest precondition dictionary generated by the static analysis,
the runtime verification logic is now able to determine the 
weakest precondition for shadow operations as they are
produced, and use this to 
classify each shadow operation as being either blue or red.

%However, to allow the runtime logic to associate a particular shadow operation to its template, we need to determine the execution path of the transaction.

The challenge with performing this lookup is that it require determining
the identifier  of the template corresponding to the path taken. However, the
path taken at runtime is an expansion of the path used as an
identifier for the shadow operation templates. In particular, when the code contains loops,
the signature of some CRDT operations may appear more than once.
% Therefore, there is an additional challenge of identifying the correct template that is associated with a particular shadow operation.  

%{\em [Too verbose: need to explain the high level idea, not the details.]}
%The ideal solution is to keep track of the path information while executing a transaction. The path
%information essentially contains the identifiers of code blocks the execution goes through, as
%what the static path identifier does. The path matching algorithm works as follows: Given a set
%of static paths $SP$, and an actual path $acp$, we first remove all repetitions in $acp$, and then
%we perform an equivalence test on the set of code block identifiers of $acp$ and any $sp$ from $SP$.
%If the test succeeds, then a matching static path is found.
%
%{\em Move to implementation...}
%However, the current implementation is not strictly following the above strategy. The reason
%for not using the control flow information to do lookup is because of the challenges to obtain
%the path information at runtime. The one approach is to change the code to record the control flow
%when it executes either a branch or a loop statement. This solution is not transparent to programmers.
%The second approach is to instrument the code to do this recording, however, instrument will change
%the code block identifier, since we use the location of code block as its identifier. The last
%solution is to modify JVM to obtain this information, and apparently it requires much work.

To address this, for every statically generated template, we create an automata that encodes the template identifier, where the alphabet is the set of relevant CRDT operation signatures and transitions correspond to consuming a single CRDT operation signature. When a shadow operation is produced at runtime, we generate its identifier by concatenating the signatures of all CRDT operations. To execute the weakest precondition lookup, we first use the automata to match the current shadow operation identifier to a template identifier. To do this we have modified these automata to enable them to handle repetitions of CRDTs operations signatures. When an automata execution reaches its final state successfully, it implies that the shadow operation is associated with the template identifier used to generate that automata, which allows us to extract the corresponding weakest precondition from the dictionary.

% then becomes an automata execution with
%the shadow operation identifier as the input. We successfully identify
%a matching template if the execution of its automata reaches the final state 
%when the sequence of signatures in the input has been consumed in total. Due to the fact that static identifiers
%are loop-free, we apply a minor change to the automata execution algorithm to handle repetitions by 
%allowing a search to go backwards to the states it already walked through, when it cannot proceed forward.

\fi
%\vspace{-0.5cm}
%\paragraph{Weakest precondition check.}
\noindent\paragraph{Weakest precondition check.} Finally, once the weakest precondition for the template that corresponds to
a particular shadow operation is retrieved, we evaluate that precondition
against the CRDT parameters of the shadow operation. This is achieved by simply
replacing the variables in the precondition with their instantiated values and
evaluating the final expression to either true or false. If the weakest
precondition is evaluated to true the shadow operation is labeled blue,
otherwise the shadow operation in labeled red.

After this step, the shadow operation is delivered to the replication layer,
which replicates it using different strategies according to its classification,
namely red (blue) shadow operations needs (no) coordination. The replication layer
we use is the Gemini system (seen in Section~\ref{ch:redblue:sect:gemini}) we built in the RedBlue consistency framework with two following minor changes:
a) make the proxy library use \tool\ instead of manually created shadow operations;
and b) make the data writer code be able to decode and automatically execute generated shadow operations.

%A template is a transformation of the original transaction, where
%all operations are transformed in CRDT operations while
%abstracting the parameters of these operations and the state over which 
%these operations are executed. A shadow operation is therefore an
%instantiation of the corresponding template, where the CRDT operations parameters
%and the observed state are materialized.

%Given the set of relevant templates for an application, the static component
%of \tool\ must compute the combinations of operation parameters and 
%database initial state that lead a shadow operation being instantiated from
%a particular template to be invariant-preserving. As discussed above, this 
%computations in unfeasible in practice due to the unbounded nature of both
%parameters and state. To overcome this challenge, we rely on a programming
%technique --- weakest precondition computation --- which enables us to 
%statically compute, for each generated template, the weakest precondition
%that ensures the invariant preservation property. These weakest preconditions
%are then stored on a dictionary using as key the identifier of the template 
%(i.e., transaction name) as to provide an efficient mechanism for the runtime
%component of \tool\ to access and verify the weakest precondition for 
%instantiated shadow operations.

%Preconditions are then stored on a dictionary using as key the execution 
%path identifier that denotes that particular template.

%However, this still does not fully address the problem depicted
%in Figure~\ref{fig:codeOriginal}. This is because,
%in presence of loops in the generator operation, the
%enumeration of shadow operations is not possible, since an unbounded
%number of loop iterations would lead to an infinite set of control
%flow paths.
%To address this problem, we observe that in many cases, including
%all the applications we analyzed, these loop iterations are independent.
%By independence, we mean that the states modified by each iteration
%are disjoint. For instance, the \texttt{doBuyConfirm} transaction whose
%simplified code is shown in Figure~\ref{fig:codeDoBuy}
%iterates over a set of items, and performs changes to an item in each iteration.
%This observation allows us to simplify a loop by only having to consider the conditions
%for a single iteration to maintain the invariant preservation property -- these
%conditions are then validated against all the iterations in the loop at runtime, and,
%given the independence property,
%this validation composes for the entire loop execution.
%In practice, this means that our analysis will only need to consider 
%two distinct paths: 
%either the loop is not taken at all, i.e, a path preserving
%all statements in that transaction but excluding the loop, 
%or the loop body is executed exactly once, 
%i.e, a path preserving all statements in that transaction but 
%replacing the loop with its body.

%This modification, however, raises the challenge that there is no
%longer a one to one mapping between the code used in the static analysis
%and the code that is generated at runtime.  Instead, each weakest precondition
%computation will be mapped to multiple
%execution paths where all simplified loops are expanded. As such, this
%complicates the runtime analysis, which now needs to find the appropriate
%match and apply the weakest precondition check to every loop iteration.

%The output from the static analysis is a dictionary consisting of 
%a set of $<key, value>$ pairs, one for each previously generated shadow operation template, and where $key$ 
%is the unique identifier of the template (i.e., the transaction name),
%and $value$ is the (simplified) weakest precondition for the template.

\if 0
The classification of shadow operations at runtime is achieved by evaluating 
the weakest precondition of its corresponding template using the concrete parameters
and relevant state values used to instantiate it. Hence, if the weakest precondition
is evaluated to \texttt{true} the shadow operation is labeled as blue, otherwise if
the weakest precondition is evaluated to \texttt{false} the shadow operation is 
labeled as red.

In the following, we discuss in more detail the main steps of the static component of \tool\ in Section~\ref{sect:staticanalysis}
and its runtime verification in Section~\ref{sect:runtimeeval}.
\fi

% To understand the limitations of this approach,
% consider the example initial operation shown in Figure~\ref{fig:codeOriginal}.
% To compute the desired conditions, we would analyze the corresponding
% generic shadow operation shown in Figure~\ref{fig:codeShadow} to check
% under which parameter values the generic shadow operation is invariant
% preserving (i.e., under which combinations of initial state seen by the
% generator operation and parameters to this operation).
% Such an analysis, however, is not scalable because the analyzer must consider
% all possible $2^{10}$ code paths. As a fallback, the analyzer ends up with
% a conservative condition that labels too many operations as strongly
% consistent. Alternatively, some tools require programmers to
% provide appropriate loop invariants to guide the proof~\cite{Kuncak2007Jahob}, 
% thus requiring undesirable human intervention in this process.
% 
% To address these limitations, we rely on static analysis to construct
% a set of templates describing all possible sequences of CRDT
% operations, while abstracting the concrete parameters associated with
% these operations.
% that can be included in a shadow operation that is generated by
%a given initial operation. This allows us to build a dictionary, which
%is looked up at runtime, constructed by analyzing the
%generator operation code in order to enumerate all possible paths
%taken by the execution of the generator operation, which maps to all
%possible sequences
%of CRDT operations that can be produced.  
%Shadow operation templates are then analyzed by a verifier,
%which computes
%for each template a weakest precondition that ensures the invariant
%preservation property.
%, i.e., that applying the shadow operation to any
%state of system will lead to a final state where all invariants hold. 

\if 0
\subsection{Static Analyzer}
\label{sect:staticanalysis}
The core of \tool\ is a static analyzer to
offer opportunities for a light-weight runtime
check. Here, we describe its design choices, 
challenges and optimizations.

\subsubsection{Generating Templates}
To create a template per original transaction, we have
to take into account the following requirements. First, due to the definition
of correct shadow operation in Section~\ref{sect:backredblue}, the template
must preserve all control flow information contained within the original transaction. 
Second, the key intuition behind shadow operations is to offer deterministic replication.
To ensure this, templates should not read states from the database, instead they must remove all non-deterministic sources 
from the original code. The template must be complaint with the key features of shadow operations
as shown in Section~\ref{sect:commute}, meaning that operations must commute by design, and also 
encode merge semantics %\joao{ALREADY ADDRESSED! Cheng, please verify if this last sentence still means what you originally intended}. 
%Consequently, templates also have to reflect
%these intended semantics. 

To meet the above requirements, for each transaction, its corresponding template
is created by duplicating the original code, and replacing all state fetched from database
by the original code by parameters of the template. Additionally, all state mutations 
in the original code are replaced by corresponding CRDT operations. In order to illustrate this 
transformation, we rely on the example example depicted in Figure~\ref{fig:codeDoBuy}.
The original transaction (Figure~\ref{fig:codeOriginal}) follows
a common pattern in one of our case-study applications, mimicking
the non-invariant-preserving \texttt{doBuyConfirm} operation
of the TPC-W benchmark. The corresponding template generated from this code is depicted in Figure~\ref{fig:codeShadow}.
This example illustrates the fact that \texttt{for} and \texttt{if-else} control flows
remain unaltered in the generated template. While the parameter \texttt{obsX} of the template encodes the database state fetched by
the original code and \texttt{deltaA} records side effects. The merge semantics are implicitly encoded in the employed CRDT operations, which in the
example relies on the type \texttt{NumerDelta}. 

\subsubsection{Weakest preconditions}
\fi



\if 0

The following step in the static component of \tool\ is to compute a weakest precondition
for each generated template, capturing the set of template parameters that generate
invariant-preserving shadow operations.
We rely on Jahob~\cite{Kuncak2007Jahob}, a verification tool
for Java programs, to compute these weakest preconditions.
Jahob requires two inputs: a set of
specifications that abstract state transitions and the application-specific invariants
that constraint valid system states.

To address the first requirement,
we manually specified the entire CRDT library, note that these specifications can be used across
different applications that rely on the CRDT types defines in this library. 
This contrasts with verifying generic Java programs, which requires the programmer to manually
specify the functions and data structures of their programs~\cite{Kuncak2007Jahob}.
% In contrast to the
% one-time manual process, our tool will automatically produce 
% correct specifications based on the annotations given by programmers.

For the second requirement, programmers must provide the application-specific invariants manually, since they encode the programmer intentions regarding the set of admissible behaviors of their application.
%In theory, we could infer invariants from code, instead of manually discovering. However,
%it is not our primary concern. We leave this as future work. 
Table~\ref{tab:staticinvariants}
provides a set of example of invariants that we manually specified for our case-study applications.
%, while
%the corresponding parts of templates and the database data structures that use CRDTs 
%are automatically produced. 

%Figure~\ref{fig:staticlwwspecs} depicts the code and specifications of a \texttt{LwwInteger} CRDT, which
%deploys a last-writer-win strategy. Figure~\ref{fig:staticdeltaspecs} shows the code and
%the specifications of a \texttt{NumberDeltaInteger} CRDT, which always applies a $delta$ to
%its current value. Table~\ref{tab:staticinvariants} shows all identified invariants for different applications.

%As stated before, we rely on Jahob to compute the weakest precondition. Figure~\ref{fig:staticJahob}
%shows the interface design and employed algorithms in more detail. 
After providing these inputs to Jahob for each generated template, Jahob outputs a weakest precondition in the form of a set of logic formulas. 
Unfortunately, the output of Jahob cannot be used directly since it contains a large number
of formulas that are either not relevant to ensure the invariant-preservation property of shadow operation, or that are indisputably true, leading
to unnecessary overheads for the runtime check component. To circumvent this, the output of Jahob
is simplified to eliminate these logic conditions.
More fundamentally, any constraints over the state against which the shadow operation
is applied must be strengthened to meet the definition of invariant
preserving operations, which requires that transitioning
from a valid state to another valid state is valid against any
possible initial state.
However, labeling any logical constraint over the initial state as \texttt{false} could lead
\tool\ to conservatively label operations as red, when they could in fact be labeled blue. To avoid this,
we first try to see if such logical constraints can be eliminated by deriving from any of the
preconditions or invariants over the state. If this is not possible, then the logical constraint is replaced by \texttt{False}, to conservatively assume any possible state.
%{\em Cheng, I replaced a long explanation with the last sentence, can you check if it captures what you meant?}

\if 0
 Additionally, the Jahob output 
may encode constraints over the system state which is shared across replicas. {\em Did not understand the last sentence, plus this seems too detailed.} This particular
constraints should either be safely removed or statically evaluated to be \texttt{False}. By safe
removal, we mean that we should strengthen the set of conditions to exclude system state by
taking into consideration known restrictions over the state such as the ones denoted by invariants. 
Note that even if the shadow operation makes direct observation of the system state, these values 
cannot safely be used at runtime to evaluate the weakest precondition, as they can change due to 
the execution of concurrent operations. Due to this, if conditions over the system state cannot be
removed, they should be statically evaluated to \texttt{False}, which means that any shadow operation
that fits that template should always be classified as Red.
\fi

%{\em RR: Suggest removing this entire paragraph, since the example relates to RedBlue, not the tool design.}
%As an illustrative example, consider a transaction that creates a new user with an unique username. 
%The invariant shown in Table~\ref{tab:staticinvariants}
%states that if there are two users in the table with the same username, then the two users
%must be the same person. Given an input that is a user record $u$, then the weakest precondition
%of this shadow operation will be $\forall u' \in usertable. u'.username \not= u.username$. Assume there exists
%two transactions $T1$ and $T2$ operating on two replicas $r1$ and $r2$, and both of them want to
%create a new user with name $jessie$. If both transactions were classified as blue, then both could
%be executed locally, and then lazily propagated to the other replica. However, when both shadow operations
%are executed by the remote replicas, the system would evolve to an invalid state. 

%There are two problems with this runtime checking. First, we
%are able to inspect all records in this table. Second, even we can load all records in the memory, 
%we still cannot fully trust the state we read. Suppose the both transactions will successfully 
%determine that there no exists a user record named $jessie$, and will commit. After exchanging,
%the operation application will transition system to a invalid state where two users have the same name.

\subsubsection{Challenges}
While designing the static component of \tool\ we have identified two major
challenges that we now discuss:

\paragraph{Handling loops: } The computation of weakest preconditions would become
significantly more complex if the transaction code contains loops, as exemplified in Figure~\ref{fig:codeOriginal}.
In this case, the possible number of different control flow paths taken 
while executing the transaction is potentially unbounded, 
if there is no way to limit the maximum number of iterations 
that can happen in a loop. This would deal out tool to conservatively label any such 
operation as read (i.e., strongly consistent) leading to a penalty in terms of performance.
An alternative to circumvent this issue, would be to leverage previous work\,\cite{Kuncak2007Jahob}, which 
requires programmer to specify loop invariant to guide the proof. This would however require
additional and error prune human intervention.

To address this challenge, we note that in many cases (including all
applications that we analyzed), loop iterations are independent, in the
sense that the fractions of the state modified in each iteration are disjoint.
This is illustrated by the example in Figure~\ref{fig:codeDoBuy} 
(which mimics the non-invariant-preserving \texttt{doBuyConfirm} operation
in the TPC-W benchmark). In the example, the loop is used to iterate over a set of items, and in each iteration
it only modifies state that is related to that particular item.

We note that the loop independence property enables us to significantly simplify the handling of loops. 
When generating the weakest precondition associated with a loop, we only have to
consider two individual control flow paths, one where the code within the loop is
never executed, and another where the code within the loop is executed once.
This condition can then be validated against each individual iteration of the loop at runtime and,
given the independence property, this validation will be valid for the entire loop execution.
We note that we have verified all loops in our case-study application and in all of 
them loop iterations were indeed independent. This process however could be
performed in an automatic fashion, by leveraging existing previous work~\cite{}. We
however plan to address the implementation of these techniques as future work.

\paragraph{Finer grained templates: }
A shadow operation only exercises a single execution path
of the original transaction, while a template captures all distinct possible execution paths
of the original transaction. The different
granularity between templates and shadow operations introduces two challenges. 
First, because templates encode all possible execution paths, the size of
computed weakest precondition grows significantly as the number of possible 
execution paths grows, for instance, in the presence
of the mix of conditional branches~\cite{Flanagan2001CompactVC}. 
As we discuss in the evaluation section, some transactions exhibit a complex structure
with many possibles distinct paths. In this cases,
a big precondition is generated, which leads to more complex runtime verification\cheng{~Check two
things: a) time-consuming to evaluate (heavy-weight); and b) will be simplified to false (not minimizing
the strong consistency part).}. 
The second issue, is that these larger weakest preconditions are harder to simplify, which may lead
\tool\ to conservatively reduce these weakest preconditions to \texttt{false}, potentially leading to an
increase in the number of shadow operations (unnecessarily) labeled as red, which leads to 
performance penalties on the system. Therefore, producing
fine-grained templates is crucial to ensure low overhead and higher performance.

To achieve this, we have changed the strategy to generate templates, such that
original transaction maps to a set of templates, where each template represents one
execution path of the original transaction. Therefore, shadow operations become 
instantiations of the template that matches to the concrete execution path for the corresponding
original transaction. This requires the runtime component of \tool\ to keep track of the execution
path of operations, as to match the current shadow operation to the corresponding template.
We discuss in the following section how this is achieved in practice.

\subsubsection{Optimized Design}
\begin{table}[t!]
\centering
\begin{tabular}{|c|c|}
\hline
Reduced path abstraction& Description \\\hline
$1\cdot2$ & no loop\\
$1\cdot2\cdot3$ & taking one branch\\
$1\cdot2\cdot5\cdot6$ & taking another branch\\
$1\cdot2\cdot3\cdot5\cdot6$ & taking both branches\\
\hline
\end{tabular}
\caption{Distinct sequential paths obtained for the transaction in Figure~\ref{fig:codeOriginal}.}
\label{tab:pathreduce}
\end{table}

The designed described above for the static component of \tool\ leads to an optimized design of its run time component, in which
we first discover a set of distinct sequential paths of a transaction, and then
create template for each path. 

The algorithm used to obtain the sequential paths key mechanism is to replace loops and split branch statement.
We now provide a sketch of this algorithm: First, for every transaction,
we create a path abstraction for it, which encodes all control flow information within
that transaction in a condense fashion. Regarding the example shown in Figure~\ref{fig:codeOriginal}, 
its path abstraction is $(1\cdot2\cdot(3|(5\cdot6))*)$, where digital numbers
are statement identifiers, $\cdot$ concatenates two sequential statements,
$|$ is a binary operator that indicates the statements at its two sides
are in alternative branches, and $*$ means that the sequence of
statements that it refers to is in a loop.
Second, we recursively apply the following two steps to simplify a path abstraction until 
it is sequential (i.e., no $*$ and $\cdot$). For a path abstraction containing $*$, we create two duplicated
abstractions, where one excludes the entire loop, and the other simplifies the loop into its a single iteration.
For a path abstraction containing the operator $|$, we create two duplicated
path abstractions, where one excludes the right operand and the other
excludes the left operand. Additionally, if such $|$ is affected by
a $*$, then we have to create another path abstraction where $|$ is
replaced by $\cdot$. In the previous example, the set of sequential paths
are shown in Table~\ref{tab:pathreduce}. 

Given a set of sequential paths for a transaction, creating shadow operation templates become
straightforward. For each path, we collect a sequence of
statements specified by the identifiers in the abstraction from the corresponding control flow graph.
Then, we translate every database function call into either a CRDT operation by following
the instructions stated in Section~\ref{sect:commute}, or a no-op operation (for read queries). 
Finally, all these CRDT operations are packed into a function, which
denotes the shadow operation template.

\paragraph{Summary: }The final output from the static analysis is a dictionary consisting of
a set of $<key, value>$ pairs, one for each previously generated shadow operation template, where $key$
is the unique identifier of the template (i.e.,
%the control flow path that denotes that template
a string concatenating signatures of CRDT operations that unequivocally identifies that template),
and $value$ is the (simplified) weakest precondition for the template.

\if 0

%Next, we describe in more detail the sequence steps for generating all shadow operation templates.
\cheng{The following paragraphs could be merged into a single and simple paragraph.
At least, the control flow graph and path abstraction can be merged. We also
need to connect the path reduction back to the loop independence property.}
{\bf Control flow graph.} To create templates on a per path basis we need
to extract the control flow information of transactions. As a result, the first step
we take is to construct a control flow graph, in which vertexes are statements within
a transaction and edges are transitions between statements.
In addition, we perform an inter-procedure analysis to inline functions whenever a function call is identified in the source code, a step that is required to simplify the work of the verifier.
%To obtain the control flow graph, we take advantage of 
%javaparser\endnote{JavaParser web page: \url{https://code.google.com/p/javaparser/}.},
%a tool to generate an abstract syntax tree (AST) for a java file and supports visiting
%methods to each AST node. We use this tool to parse all {\tt .java} files that compose the application code-base into a set of ASTs. 

\if 0
{\em RR: The next paragraphs are not properly motivated.}

We then produce a tentative control flow graph for each function using the following algorithm.
{\em [RR: Is this a normal CFG or is there something special about it? If former than may be unnecessary to
explain. If latter then you need to say what is special about it.]}
We create an empty control flow graph $g$, and we keep consuming a statement $s$ from a \texttt{MethoDeclaration} 
node in an AST, until there is no statement left. For every $s$, we create a vertex $v$
for it, and add $v$ to $g$. If $s$ is the first statement of the function, then
we flag $v$ as \texttt{Entry}. Similarly, if $s$ is the last statement of the function or an explicit return statement, we flag $v$ as \texttt{Exit}. 

If $s$ is a simple statement (i.e., not a loop, if, else, throw, catch, return), then we
create a link that connects $v$ to the vertex $v^{'}$ pointing to the statement $s^{'}$
preceding $s$ if $s^{'}$ exists. If $s$ is an if/else statement, then we create and add to $g$
three vertexes $v_0$, $v_1$ and $v_2$ that represent the logical expression that composes the if, as well as the true and false branches, respectively. 
Both $v_1$ and $v_2$ are $v_0$'s children, and $v_0$ is then connected to the vertex pointing
to its preceding statement (if it exists). Upon the inclusion of such a branch, we proceed with the analysis for each
branch independently to recursively analyze all 
statements contained in if and else branches, respectively. The final output of a process will
yield another control flow graph $g'$ for each branch, and $g'$ will be merged into $g$.
If $s$ is a loop statement, then we construct a control flow graph $g''$ for the loop. Again,
we have to merge $g''$ with $g$ as well.   

The next step that is required is to inline functions in the tentative control flow graph.
The reason why function inlining is required is that Jahob, the tool we use to compute
the weakest precondition, does not support inter-procedural analysis. 
Inlining a function $f$ into another caller function $f_c$ is achieved by:
$1)$ find the vertex $v$ where $f$ is called in $f_c$'s control flow graph $g_c$;
$2)$ find the set of ancestors of $v$, denoted as $P$, and the set of successors of $v$, denoted as $S$; 
$3)$ finding the \texttt{Entry} vertex $e$ in $f$'s control flow graph $g$ and the set of \texttt{Exit} vertices $E$; 
$4)$ break all of $v$'s fan-in links, and link all vertices in $P$ to $e$; and finally, 
$5)$ connect all vertices in $E$ to $v$.
\fi

%The control flow graph obtained by running the above algorithm is tentative, since
%the functions called in that graph have not been inlined. 
%{\em [RR: The following text might be too detailed]}
%To inline functions, we search for
%a candidate function by checking the scope of the function invocation, the name of callee
%function, the number of arguments, and the argument type to handle some corner cases: a)
%functions in different packages with the same name; and b) overloaded functions. This search
%will be performed through the whole project. For those functions from a library whose code is
%not visible, we are not able to inline them. {\em [RR: so what can we do?]}
%Inlining a function $f1$ to another function $f2$
%is done as follows: a) find the vertex $v$ where $f1$ is called in $f2$'s control flow
%graph $g2$; b) find the set of ancestors of $v$, denoted as $P$, and the set of successors of $v$, denoted as $Su$; 
%c) finding the \texttt{Entry} vertex $ent$ in $g1$ and the set of \texttt{Exit} vertices $Ex$; d) break
%all of $v$'s fan-in links, and link all vertices in $P$ to $ent$; and e) connect all vertices in $Ex$ to $v$.

{\bf Path Abstraction.}
To generate path identifiers we need
to manipulate the control flow graph of each
transaction to replace loops and split branch statements.
Doing this at the control flow graph level is
expensive, since we have to duplicate graphs. Therefore, we 
first extract a regular expression that summarizes
all control flow information, and then we operate on that expression. We define
this regular expression as a path abstraction, which is a sequence 
of statement identifiers separated by three reserved key symbols, namely $\cdot$,
$*$, and $|$. $\cdot$ concatenates two sequential statements,
$+$ is a binary operator that indicates the statements at its two sides
are in alternative branches, and $*$ means that the sequence of
statements that it refers to is in a loop. For example,
$(1\cdot2\cdot(3|(5\cdot6))*)$  denotes two operations (1 and 2) and an if/else pair statement inside
a loop (the if branch is composed by operation $3$, and the else branch by operations $5$ and $6$). Given a control flow graph, we extract its corresponding
path abstraction using a 
transitive closure computation algorithm.
\if
The second step towards obtaining the finite set of paths is to
extract a path abstraction from the control flow graph $g$ as follows. First, we label every vertex and edge
in $g$ with unique integer identifiers. Then, we convert $g$ into 
a matrix $m$, representing the connectivity of $g$. Third, we run the 
transitive closure computation algorithm~\ref{alg:regularPaths}, obtaining an output in the form 
of a regular expression that denotes all possible path such as $(1.2.(3|(5.6))*)$. Where $|$ encodes two alternative paths that cannot happen simultaneously and $*$ encodes a set of steps which are within a loop, meaning that they might appear either zero or one time in a concrete path. {\em [Explain notation?]}
\fi

{\bf Path reduction.} 
%Given the regular expression obtained through the use of path abstraction,
%one has to extract the set of paths such that the sequences of CRDT operations obtained by traversing 
%each path form the a template shadow operation which weakest precondition can then be computed.
Next, we reduce the
path abstraction to a set of paths only containing a sequence of statement identifiers concatenated
by the operator $\bullet$, by splitting branches and replacing loops as
mentioned. To transform a non-reduced path abstraction to a set of reduced ones,
we employ the following algorithm: $1)$ for a path abstraction
containing the operator $+$, we create two duplicated
path abstractions, where one excludes the right operand and the other
excludes the left operand. Additionally, if such $+$ is affected by
a $*$, then we have to create another path abstraction where $+$ is
replaced by $\bullet$; $2$) for a path abstraction containing $*$,
we also create two duplicated path abstractions, where one excludes
the entire operand, and the other just simply removes $*$. The algorithm is
recursively applied if needed, and terminates
when all path abstractions are in the reduced format.
In the previous example, the set of reduced path abstractions 
for $(1\bullet2\bullet(3+(5\bullet6))*)$ are shown in Table~\ref{tab:pathreduce}.


{\bf Template creation.}
Given a set of reduced paths, creating shadow operation templates become
straightforward. For every reduced path abstraction, we collect a sequence of
statements specified by the identifiers in the abstraction from the corresponding control flow graph.
Then, we translate every database function call into either a CRDT operation by following
the instructions stated in Section~\ref{sect:commute}, or a no-op operation (for read queries). 
Finally, all these CRDT operations will be packed into a function, which
is the template we use.
%considering a transaction $T$, its control flow graph $g$, and for each path $p$ in its set of reduced paths $P$, we 
%first obtain a sequence of statements $S$ from $g$ by copying every statement pointed by
%a vertex in $p$ into $S$. Then, by following the commutativity transformation,
%we generate a shadow template $T_s$ that has $p$ as its unique identifier, which is obtained by
%concatenating the identifiers of code blocks along that control flow path that generate that template.
%These templates are then used to compute weakest preconditions as we detail below.

This modification, however, raises the challenge that there is no
longer a one-to-one mapping between the code used in the static analysis
and the code that is generated at runtime. Thus we need an extra logic
at runtime to find the appropriate mapping.
\fi


\if 0

\begin{figure*}[!ht]
\begin{minipage}[b]{.48\textwidth}
\begin{subfigure}[Original code]{
\pseudocodeinput[breaklines=true,mathescape=true]{code/forLoopExample.txt}
\label{fig:codeOriginal}
}
\end{subfigure}
\end{minipage}
\begin{minipage}[b]{.48\textwidth}
\begin{subfigure}[Possible corresponding shadow template]{
\pseudocodeinput[breaklines=true,mathescape=true]{code/forLoopShadowExample.txt}
\label{fig:codeShadow}
}
\end{subfigure}
\end{minipage}
\caption{Code snippet of a transaction and a possible template for the corresponding shadow operation.}
\label{fig:codeDoBuy}
\end{figure*}

\subsection{Computing weakest precondition on shadow operations}


\subsection{Static analysis: building the dictionary}

At runtime, every invocation of a generator operation produces a
shadow operation comprised of a sequence of CRDT operations, which
must be labeled according to whether it meets the invariant
preservation property.

As mentioned in Section~\ref{sect:overview}, a possible solution to
minimize the runtime checks that need to be performed would be to
statically compute the combinations of operation type and parameters,
and initial states that lead to generating an invariant-preserving
shadow operation. To understand the limitations of this approach,
consider the example initial operation shown in Figure~\ref{fig:codeOriginal}.
To compute the desired conditions, we would need analyze the corresponding
generic shadow operation shown in Figure~\ref{fig:codeShadow} to check if it meets
the conditions in the definition of invariant-preserving operations. This
analysis is, however, non-scalable because the analyzer would have consider
all possible $2^{10}$ code paths. In most tools, the analyzer would end up with
a conservative condition that labels too many operations as strongly
consistent. Alternatively, {\em [is this really an alternative or something
that is required in addition???]} verification tools can require programmers to
provide an appropriate loop invariant to guide the proof, thus requiring an
undesirable human intervention in this process.

To address these limitations, we construct set of templates describing all possible sequences
of CRDT operations that can be included in a shadow operation that is generated by
a given initial operation. This allows us to build a dictionary, which
is looked up at runtime, constructed by analyzing the
generator operation code in order to enumerate all possible paths
taken by the execution of the generator operation, which maps to all
possible sequences
of CRDT operations that can be produced.  Then, for each possible
shadow operation, we compute the weakest precondition over the
parameters of the CRDT invocations that ensures the invariant
preservation property, i.e., that the shadow operation leaves the
system in a final state where invariants hold.
% This condition is called weakest precondition, which is a conjunction of a set of formulas. 
The sequence of CRDT operations and the respective weakest
precondition are then be used as the dictionary key at runtime. 
{\em [RR: Or do we want to say that the sequence of ops is the key and
the weakest precondition is the value that is looked up?]}

However, this still does not fully address the problem depicted
in Figure~\ref{fig:codeOriginal}. This is because,
in presence of loops in the generator operation, the
enumeration of shadow operations is not possible, since an unbounded
number of loop iterations would lead to an infinite set of control
flow paths.
To address this problem, we observe that in many cases, including
all the applications we analyzed, these loop iterations are independent.
By independence, we mean that the states modified by each iteration
are disjoint. For instance, the \texttt{doBuyConfirm} transaction whose
simplified code is shown in Figure~\ref{fig:codeDoBuy}
iterates over a set of items, and performs changes to an item in each iteration.
This observation allows us to simplify a loop by only having to consider the conditions
for a single iteration to maintain the invariant preservation property -- these
conditions are then validated against all the iterations in the loop at runtime, and,
given the independence property,
this validation composes for the entire loop execution.
In practice, this means that our analysis will only need to consider 
two distinct paths: 
either the loop is not taken at all, i.e, a path preserving
all statements in that transaction but excluding the loop, 
or the loop body is executed exactly once, 
i.e, a path preserving all statements in that transaction but 
replacing the loop with its body.

This modification, however, raises the challenge that there is no
longer a one to one mapping between the code used in the static analysis
and the code that is generated at runtime.  Instead, each weakest precondition
computation will be mapped to multiple
execution paths where all simplified loops are expanded. As such, this
complicates the runtime analysis, which now needs to find the appropriate
match and apply the weakest precondition check to every loop iteration.

Next, we describe in more detail how we implemented the various steps of the
static analysis: deriving the dictionary of possible sequences of CRDT
operations, computing weakest preconditions for each sequence to be
invariant preserving, and building the dictionary.

%We also collapse all
%possible shadow operations corresponding to a static path into a template, which
%also contains a sequence of CRDT operations as each shadow operation does but excluding
%these repetitions of CRDT operations due to unfolding the loop. After this step,
%we analyze every template and produce a weakest precondition for each template.
 
\if 0
Assuming that
the transactions we want to analyze are sequential, i.e., no conditional and loop statement,
the weakest precondition computation is very straightforward by iteratively computing
weakest precondition of every statement from the exit to the entry. However, if the code
structure of a transaction gets more complex, for example, it may contain conditional and loop
statements, then the size of conjunctive formulas will significantly increase, since the branch
and iteration information need to be taken into account. Specially, analyzing loops is an undecidable
problem as the number of iterations is normally unbounded. For this sake, many verification tools like 
Jahob require programmers to provide a proper loop invariant to guide the proof, instead of
inferring on its own. 

Clearly, the complexity of static analysis and runtime verification is proportional to 
the complexity of the corresponding transaction\cheng{have to check}. If we cannot trim
down the size of weakest preconditions, then we will end up with the fact that we are
not able to efficiently verify these conditions at runtime. This influences the identification of
eventually consistent operations, and the system performance. As a result, reducing the complexity
of the code fed into the static analysis becomes very imperative. One thing we can do
is to break a complex transaction into a finite set of distinct paths. Note that the definition of
path is a linear flow of code blocks (a few sequential statements) starting from the entry block and 
terminating at the exit block. In other words, a path doesn't contain loops and branches. 

Reducing a transaction only containing if/else statement is to simply create the one path
that preserves all statements in that transaction but only excludes the statements within else
branch, and the other vice verse. In contrast, reducing loops into a finite set of paths is non-trivial,
since we cannot arbitrarily determine the number of iterations. When carefully inspecting the source
code of applications, we discovered that each iteration within a loop is independent from all other iterations.
By \textbf{independence}, we mean that all states read or modified at an iteration don't
rely on states from all previous iterations and will not influence the subsequent iterations.
This observation allows to simplify a loop into two distinct paths: a) the one path doesn't enter
into loop at all, i.e, a path preserving all statements in the corresponding transaction but
excluding the entire loop; and b) the other path exercises the loop body once, i.e, a path
preserving all statements in that transaction but replacing the loop with its body. For the mix
of loop and if/else, we will detail the explanation next.

\fi

\subsubsection{Implementation}

{\bf Path discovery.}
We identify the possible sequences of shadow operations by the path
that was taken by the corresponding initial operation. Therefore
the goal of this step is to identify the distinct control flow paths of
a generator operation, and to replace loops with the alternative paths
of either not entering the loop or entering it once. To this end, we
%Given the code of a transaction, to identify its distinct paths, we have
%to first understand the code structure, to summarize all possible flow
%information, and to remove loops and if/else as needed. In this case,
%we design three pieces of logic for the distinct path discovery functionality, namely 
generate a control flow graph, extracting the corresponding path abstraction, and
reduce this path abstraction into a set of paths.

%We already give the definition of
%path above, and will only define the rest two concepts here.

%A control flow graph is a directed graph (may be acyclic) in which each vertex
%is a basic code block (in our implementation, it is a statement) and each edge
%indicates a possible flow of control. A path abstraction is a regular expression
%over an alphabet set of operators like *, + and ., and the identifiers of code 
%blocks, and it condenses all possible control flows of a transaction. The meaning of
%these three operators are as follows: * means that the code block preceding it is
%a loop, + means that two code blocks at its two sides are branches, and . means
%that the two code blocks at its two sides will be always executed sequentially together.

{\bf Control flow graph.} To obtain the control flow graph, we take advantage of 
\href{https://code.google.com/p/javaparser/}{JavaParser},
a tool to generate an abstract syntax tree (AST) for a java file and support visiting
methods to each AST node. We use this tool to parse all {\tt .java} files within a given project into a set of ASTs. Next,
we produce a tentative control flow graph for each function using the following algorithm.
{\em [RR: Is this a normal CFG or is there something special about it? If former than may be unnecessary to
explain. If latter then you need to say what is special about it.]}
We create an empty control flow graph $g$, and we keep consuming a statement $s$ from a \texttt{MethoDeclaration} 
node in an AST, until there is no statement left. For every $s$, we create a vertex $v$
for it, and add $v$ to $g$. If $s$ is the first statement of the function, then
we flag $v$ as \texttt{Entry}. If $s$ is the last statement of the function without
return statement, or it is a return statement, then we flag $v$ as \texttt{Exit}. If $s$
is a simple statement (i.e., not a loop, if, else, throw, catch, return), then we
create a link that connects $v$ to the vertex $v^{'}$ pointing to the statement $s^{'}$
preceding $s$ if $s^{'}$ exists. If $s$ is a if/else statement, then we create and add to $g$
three vertexes $v0$, $v1$ and $v2$ representing the comparison statement, if and else branches, respectively. 
Both $v1$ and $v2$ will be $v0$'s children, and $v0$ will be connected to the vertex pointing
to its preceding statement if necessary. After that, we fork into two processes to recursively analyze all 
statements contained in if and else branches, respectively. The final output of a process will
yield another control flow graph $g'$ for each branch, and $g'$ will be merged into $g$ at proper places.
If $s$ is a loop statement, then we construct a control flow graph $g''$ for the loop. Again,
we have to merge $g''$ with $g$ as well.   

The next step that is required is to inline functions in the tentative control flow graph.
The reason why function inlining is required is that the tool we use to derive
the weakest precondition does not support interprocedural analysis.
%The control flow graph obtained by running the above algorithm is tentative, since
%the functions called in that graph have not been inlined. 
{\em [RR: The following text might be too detailed]}
To inline functions, we search for
a candidate function by checking the scope of the function invocation, the name of callee
function, the number of arguments, and the argument type to handle some corner cases: a)
functions in different packages with the same name; and b) overloaded functions. This search
will be performed through the whole project. For those functions from a library whose code is
not visible, we are not able to inline them. {\em [RR: so what can we do?]}
Inlining a function $f1$ to another function $f2$
is done as follows: a) find the vertex $v$ where $f1$ is called in $f2$'s control flow
graph $g2$; b) find the set of ancestors of $v$, denoted as $P$, and the set of successors of $v$, denoted as $Su$; 
c) finding the \texttt{Entry} vertex $ent$ in $g1$ and the set of \texttt{Exit} vertices $Ex$; d) break
all of $v$'s fan-in links, and link all vertices in $P$ to $ent$; and e) connect all vertices in $Ex$ to $v$.

{\bf Path Abstraction.} The second step towards obtaining the finite set of paths that are used
as a dictionary is to
extract a path abstraction from the control flow graph $g$ as follows. First, we label every vertex and edge
in $g$ with unique integer identifiers. Then, we convert $g$ into 
a matrix $m$, representing the connectivity of $g$. Third, we run the 
transitive closure computation algorithm~\ref{alg:regularPaths}, obtaining an output of the
form, e.g., $(1.2.(3+(5.6))*)$. {\em [Explain notation?]}

{\bf Path reduction.} The goal of this step is to convert the path abstraction into a set
of paths such that the sequences of CRDT operations obtained by traversing each path form
the template shadow operations whose weakest precondition we need to compute.
{\em [RR: The following explanation is cryptic and contains no high level rationale, need to rewrite]} 
There are a few steps to do this. Given a path 
abstraction $pa$, if it contains $*$, then we break it into two intermediate forms: 
$pa'$ without $*$ and the sub-expression affected by $*$, and  $pa''$ just without the operator $*$. 
If the sub-expression also contains $+$, then $pa''$ will be further reduced into three other intermediate
forms: a) one preserving all information in $pa''$ but excluding the right hand operand of the $+$; b)
one preserving all information in $pa''$ but excluding the left hand operand of the $+$; and c) one preserving
all information in $pa''$ but only replacing that $+$ with a $.$. 
Once all $*$ are eliminated from all intermediate forms, 
then we move to the second step to fork new paths for each branch statement. To this end, we scan all intermediate
forms, checking it has $+$. If so, then we break it into two another intermediate forms,
each of which goes through one conditional case. If there exist nested $*$ or nested $+$ commands, then
the above two steps can be applied nestedly. The algorithm terminates when
all intermediate forms have no loops and branches. The set of paths
reduced from the path abstraction $(1.2.(3+(5.6))*)$ are shown in Table~\ref{tab:pathreduce}.

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|}
\hline
Path & Description \\\hline
$1\bullet2$ & not entering loop\\
$1\bullet2\bullet3$ & in loop and taking one branch\\
$1\bullet2\bullet5\bullet6$ & in loop and taking another branch\\
$1\bullet2\bullet3\bullet5\bullet6$ & in loop and taking both branches\\
\hline
\end{tabular}
\caption{Distinct paths obtained for an exemplified path abstraction $(1.2.(3+(5.6))*)$.}
\label{tab:pathreduce}
\end{table}

{\bf Weakest precondition computation.}
Recall that we are not going to check the original code for computing its
weakest precondition. Instead, we will look at the deterministic side effects introduced
by each path we obtained from previous subsections. 

Given a transaction $T$, its control flow graph $g$, and its set of paths $P$, for each path $p$, we 
first obtain a sequence of statements $S$ from $g$ by copying every statement pointed by
a vertex in $p$ into $S$. Then, by following the commutativity transformation,
we create another transaction $T'$ (it is called template) for $p$, which
consists of a sequence of CRDT operations. The template $T'$ is the piece of
code analyzed by a verifier to produce weakest precondition.
In order to obtain the weakest preconditions, we have to do two tasks in advance: a)
specifying both the CRDTs and the generated templates; and b) providing application-specific
invariants. The specifications of CRDTs and the invariants are written manually, while
the corresponding parts of templates and the database data structures that use CRDTs 
are automatically produced. 

Figure~\ref{fig:staticlwwspecs} depicts the code and specifications of a \texttt{LwwInteger} CRDT, which
deploys a last-writer-win strategy. Figure~\ref{fig:staticdeltaspecs} shows the code and
the specifications of a \texttt{NumberDeltaInteger} CRDT, which always applies a $delta$ to
its current value. Table~\ref{tab:staticinvariants} shows all identified invariants for different applications.


Jahob first computes weakest preconditions for each shadow operation template. Figure~\ref{fig:staticJahob}
shows interface design and algorithms in more detail. The weakest precondition for a template
is a set of formulas imposing some constraints on the set of CRDT objects and their operations within
that template. The output from Jahob cannot be directly used since it contains a large number
of formulas that are either not relevant to ensure the invariants, or are indisputably true. These
formulas should not be checked at runtime, therefore we should eliminate them from the final weakest
precondition. Moreover, it may contain formulas of system data objects shared across replicas. The
set of formulas must be either safely removed or statically evaluated to be \texttt{False}. By safe
removal, we mean that we should strengthen the set of formulas to exclude shared data by
consulting the information we know, for example the invariants. We cannot use the value of 
these shared objects generated at runtime to evaluate the weakest precondition, as their values
observed when the verification takes place are very likely different from those observed
when the side effects of the corresponding transaction are applied to the system state. This difference is
introduced by the fact that the state read from a replica when a transaction is running is not the most
recent one since some other replicas may update the state concurrently but the changes haven't been
propagated to that replica. In some extent we cannot remove shared data from weakest precondition, 
we have to statically make it \texttt{False}, which means that this path is always not invariant-preserving.

The one example is to create an user with unique username. The invariant shown in Table~\ref{tab:staticinvariants}
states that if there are two users in the table have the same user name, then the two users
must be the same person. Given an input that is a user record $u$, then the weakest precondition
of this creation will be $\forall u' \in usertable. u'.username <> u.username$. Assume there exists
two transactions $T1$ and $T2$ operating on two replicas $r1$ and $r2$, and both of them want to
create a user with name $jessie$. There are two problems with this runtime checking. First, we
are able to inspect all records in this table. Second, even we can load all records in the memory, 
we still cannot fully trust the state we read. Suppose the both transactions will successfully 
determine that there no exists a user record named $jessie$, and will commit. After exchanging,
the operation application will transition system to a invalid state where two users have the same name.

{\bf Path identifier.}
The final output from the static analysis is a dictionary consisting of 
a set of $<key, value>$ pairs, where $key$ is the unique identifier of a path,
and $value$ is the weakest precondition of that path. A unique
path identifier allows for an efficient runtime lookup. The way we generate the
identifier is by concatenating the identifiers of code blocks along that path. Therefore,
the path itself becomes the identifier, as shown in Table~\ref{tab:pathreduce}.
%In the runtime verification part, we will detailed the actual implementation choice.

\subsection{Runtime verification}
\label{sect:runtimeverification}

By looking up the weakest precondition dictionary generated by the static analysis,
the runtime verification logic will be able to lookup the corresponding
weakest precondition, and then determine what consistency level should be assigned
to an execution of a transaction. This decision is divided into two consecutive
steps: a) looking up the correct path to fetch weakest precondition; and b)
validate if the parameters of the shadow operation meet the weakest precondition.

The challenge with determining the path identifier to perform the dictionary lookup
is that the path taken at runtime is an expansion of the path used as an
identifier. More precisely, the runtime path is still a linear flow of code blocks, but it differentiates itself from
the static path by possibly containing repetitions, corresponding to loop iterations.  

{\em [Too verbose: need to explain the high level idea, not the details.]}
The ide solution is to keep track of the path information while executing a transaction. The path
information essentially contains the identifiers of code blocks the execution goes through, as
what the static path identifier does. The path matching algorithm works as follows: Given a set
of static paths $SP$, and an actual path $acp$, we first remove all repetitions in $acp$, and then
we perform an equivalence test on the set of code block identifiers of $acp$ and any $sp$ from $SP$.
If the test succeeds, then a matching static path is found.

{\em Move to implementation...}
However, the current implementation is not strictly following the above strategy. The reason
for not using the control flow information to do lookup is because of the challenges to obtain
the path information at runtime. The one approach is to change the code to record the control flow
when it executes either a branch or a loop statement. This solution is not transparent to programmers.
The second approach is to instrument the code to do this recording, however, instrument will change
the code block identifier, since we use the location of code block as its identifier. The last
solution is to modify JVM to obtain this information, and apparently it requires much work.

Because of the above reasons, we choose a different way to generate the path identifier for both
offline and runtime. Every static path or runtime path is mapped to a sequence of CRDT operations. Therefore
we compute the identifier of a path by concatenating the signature of all CRDT operations. The look
up algorithm works as follows:

\begin{itemize}
\item a) construct a prefix tree to store all signatures of all paths.
\item b) when a runtime path comes, starting the search from the root, go to c)
\item c) compare every sub signature of the runtime generated crdt operation along the runtime path against the value of the node in the tree, from top to leaf. go to d)
\item d) multiple situations:
   \begin{itemize}
     \item d1) if the sub signature ($s1$) of the crdt operation ($op$) matches the value ($s0$) of the currently visited node ($n$):
        \begin{itemize}
	\item if $n$ is the leaf and $op$ is last one, go to e)
        \item if ($n$ is not the leaf and $op$ is last one)  or ($n$ is the leaf and $op$ is not the last one), go to f)
        \item if $n$ is not leaf and $op$ is not last one, and the search will proceed to the child node of the visited node , and to the next crdt operation. go to c)
	\end{itemize}
     \item d2) if the comparison doesn't match, then you are very likely running into a loop. Therefore, the search must go back to the all nodes along the prefix you scanned, and to check whether there exists a node whose value matching your sub signature
	\begin{itemize}
        \item if matching, go to d1)
        \item if not matching, go to f)
	\end{itemize}
    \end{itemize}
\item e) you found ! return weakest preconditions
\item f) return not found error  
\end{itemize}

{\bf Problem with this looking up solution.} Using CRDT operations
to generate the path identifier loses path information. The missing information
may lead to a situation where multiple consecutive loops are collapsed into a single one.
This collapse requires us to reason about the iteration independence cross
multiple loops. 

\if 0

\section{Building the dictionary}
\label{sect:dictionary}

Here we describe the static/dyanamic proceses for building and then
using the dictionary.  The two phases are intimitely intertwined.


The dictionary has a couple of important components:
\begin{itemize}
\item the lookup keys
\item the contained values/expressions to be evaluated
\end{itemize}

In the rest of this section we describe the lookup keys used in the
dictionary, how they are produced at runtime, and the static analysis
used to build the dictionary in a space efficient manner.


\subsection{Lookup keys}
Many different shadow operations can be produced from the same
application code, depending on which specific values are being
modified, how many loop iterations occur, and which conditional
branches are taken.  Enumerating every possible shadow operation is
both time consuming and space inefficient.

We consequently work hard to identify {\em signatures} of shadow
operations that come from the same equivalence class.  There is a
one-to-many relationship between signatures and shadow operations
produced at run-time.   

Signatures capture....

\subsection{The red/blue check}
Dictionary contains a check that can be evaluated.

Discuss how the check is instantiated for the full shadow operation.

etc.


\subsection{Building the dictionary}

Dynamic analysis to generate the lookup key/signature and evaluate the conditions.

Key challenge is building the dictionary itself.

path analysis and reduction to something that matches the signature.

generating the pre-condition.

\fi

\fi

\fi
